{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "The Markov decision process (MDP) is a mathematical framework that helps you encapsulate the real-world. Desptite simple and restrictive â€“ the sign of a good interface â€“ a suprising number of situations can be squeezed into the MDP formalism.\n",
    "\n",
    "The MDP Entities\n",
    "An MDP has two â€œentitiesâ€:\n",
    "\n",
    "An agent: An application that is able to observe state and suggest an action. It also receives feedback to let it know whether the action was good, or not.\n",
    "An environment: This is the place where the agent acts within. It accepts an action, which alterts its internal state, and produces a new observation of that state.\n",
    "In nearly all of the examples that I have seen, these two entities are implemented independently. The agent is often an RL algorithm (although not always) and the environment is either real life or a simulation.\n",
    "\n",
    "The MDP Interface\n",
    "The agent and the environment interact through an interface. You have some control over what goes into that interface and a large amount of effort is typically spent improving the quality of the data that flows through it. You need representations of:\n",
    "\n",
    "State: This is the observation of the environment. You often get to choose what to â€œshowâ€ to the agent. There is a compromise between simplifying the state to speed up learning and preventing overfitting, but often it pays to include as much as you can.\n",
    "Action: Your agent must suggest an action. This mutates the environment in some way. So called â€œoptionsâ€ or â€œnull-actionsâ€ allow you to do nothing, if thatâ€™s what you want to do.\n",
    "Reward: You use the reward to fine-tune your action choices.\n",
    "Creating a â€œGridWorldâ€ Environment\n",
    "To make it easy to understand, Iâ€™m going to show you how to create a simulation of a simple grid-based â€œworldâ€. Many real-life implementations begin with a simulation of the real world, because itâ€™s much easier to iterate and improve your design with a software stub of real-life.\n",
    "\n",
    "The goal of this environment is to define a â€œworldâ€ in which a â€œrobotâ€ can move. The so-called-world is actually a series of cells inside a 2-dimensional box. The agent can move north, east, south, or west which moves the robot between the cells. The goal of the environment is to reach a goal. There is a reward of -1 for every step, to promote reaching the goal as fast as possible.\n",
    "\n",
    "Imports and Definitions\n",
    "First let me import a few libraries (to enable the autocompletion in later cells) and define a few important definitions. The first is the defacto definition of a â€œpointâ€ object, with x and y coordinates and the second is a direction enumeration. These are use to define the position of the agent in the environment and the direction of movement for an action, respectively. Note that Iâ€™m assuming that east moves in a positive x direction and north moves in a positive y direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "from enum import Enum\n",
    "from typing import Tuple, List\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "class Direction(Enum):\n",
    "  NORTH = \"â¬†\"\n",
    "  EAST = \"â®•\"\n",
    "  SOUTH = \"â¬‡\"\n",
    "  WEST = \"â¬…\"\n",
    "\n",
    "  @classmethod\n",
    "  def values(self):\n",
    "    return [v for v in self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Environment Class\n",
    "Next I create a Python class that represents the environment. \n",
    "The first function in the class is the initialisation function in which we can specify the width and height of the environment.\n",
    "After that I define a helper parameter which encodes the possible actions and then I reset the state of the environment with a reset function.\n",
    "\"\"\"\n",
    "class SimpleGridWorld(object):\n",
    "  def __init__(self, width: int = 5, height: int = 5, debug: bool = False):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.debug = debug\n",
    "    self.action_space = [d for d in Direction]\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.cur_pos = Point(x=0, y=(self.height - 1))\n",
    "    self.goal = Point(x=(self.width - 1), y=0)\n",
    "    # If debug, print state\n",
    "    if self.debug:\n",
    "      print(self)\n",
    "    return self.cur_pos, 0, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a Step\n",
    "Recall that the MDP interface three key components: the state, the action, and the reward. The environmentâ€™s step function accepts an action, then produces a new state and reward.\n",
    "\n",
    "The large amount of code is a consequence of the direction implementation. You can refactor this to use fewer lines of code with some clever indexing. However, I think this level of verbosity helps explain what is going on. In essence, every direction moves the current position by one square. You can see the code incrementing or decrementing the x or y coordinates.\n",
    "\n",
    "The second part of the function is testing to see if the agent is at the goal. If it is, then it signals that it is at a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGridWorld(SimpleGridWorld):\n",
    "  def step(self, action: Direction):\n",
    "    # Depending on the action, mutate the environment state\n",
    "    if action == Direction.NORTH:\n",
    "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
    "    elif action == Direction.EAST:\n",
    "      self.cur_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
    "    elif action == Direction.SOUTH:\n",
    "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
    "    elif action == Direction.WEST:\n",
    "      self.cur_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
    "    # Check if out of bounds\n",
    "    if self.cur_pos.x >= self.width:\n",
    "      self.cur_pos = Point(self.width - 1, self.cur_pos.y)\n",
    "    if self.cur_pos.y >= self.height:\n",
    "      self.cur_pos = Point(self.cur_pos.x, self.height - 1)\n",
    "    if self.cur_pos.x < 0:\n",
    "      self.cur_pos = Point(0, self.cur_pos.y)\n",
    "    if self.cur_pos.y < 0:\n",
    "      self.cur_pos = Point(self.cur_pos.x, 0)\n",
    "\n",
    "    # If at goal, terminate\n",
    "    is_terminal = self.cur_pos == self.goal\n",
    "\n",
    "    # Constant -1 reward to promote speed-to-goal\n",
    "    reward = -1\n",
    "\n",
    "    # If debug, print state\n",
    "    if self.debug:\n",
    "      print(self)\n",
    "\n",
    "    return self.cur_pos, reward, is_terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "And finally, like all of data science, it is vitally important that you are able to visualise the behaviour and performance of your agent. The first step in this process is being able to visualise the agent within the environment. The next function does this by printing a textual grid, with an x at the agentâ€™s location, a o at the goal, an @ if the agent is on top of the goal, and a _ otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGridWorld(SimpleGridWorld):\n",
    "  def __repr__(self):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(self.height)):\n",
    "      for x in range(self.width):\n",
    "        if self.goal.x == x and self.goal.y == y:\n",
    "          if self.cur_pos.x == x and self.cur_pos.y == y:\n",
    "            res += \"@\"\n",
    "          else:\n",
    "            res += \"o\"\n",
    "          continue\n",
    "        if self.cur_pos.x == x and self.cur_pos.y == y:\n",
    "          res += \"x\"\n",
    "        else:\n",
    "          res += \"_\"\n",
    "      res += \"\\n\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Environment\n",
    "To run the environment you need to instantiate the class, call reset to move the agent back to the start, then perform a series of actions to move the agent. For now let me move it manually, to make sure it is working, visualising the agent at each step. I also print the result of the step (the new state, reward, and terminal flag) for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "â˜ This shows a simple visualisation of the environment state.\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "____o\n",
      "\n",
      "(Point(x=0, y=2), -1, False) â¬… This displays the state and reward from the environment ð€ð…ð“ð„ð‘ moving.\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "x____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "x___o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_x__o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "__x_o\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "___xo\n",
      "\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____@\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Point(x=4, y=0), -1, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SimpleGridWorld(debug=True)\n",
    "print(\"â˜ This shows a simple visualisation of the environment state.\\n\")\n",
    "s.step(Direction.SOUTH)\n",
    "print(s.step(Direction.SOUTH), \"â¬… This displays the state and reward from the environment ð€ð…ð“ð„ð‘ moving.\\n\")\n",
    "s.step(Direction.SOUTH)\n",
    "s.step(Direction.SOUTH)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)\n",
    "s.step(Direction.EAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "There are a few key lessons that you should commit to memory:\n",
    "\n",
    "The state is an observation of the environment, which contains everything outside of the agent. For example, the agentâ€™s current position within the environment. In real world applications this could be the time of the day, the weather, data from a video camera, literally anything.\n",
    "The reward fully specifies the optimal solution to the problem. In real life this might be profit or the number of new customers.\n",
    "Every action mutates the state of the environment. This may or may not be observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Reinforcement Learning Solution to the MDP: Implementing the Monte Carlo RL Algorithm\n",
    "\n",
    "Rather confusingly, RL, like ML, is meant both as a technique and a collection of algorithms. Exactly when an algorithm becomes an RL algorithms is up for debate, but it is generally accepted that there has to be multiple steps (otherwise it would just be a bandit problem) and it attempts to quantify the value of being in a particular state.\n",
    "\n",
    "An algorithm called the cross-entropy method is an algorithm that attempts to stumble accross the goal. However, once it has then it replicates the same movements again to reach the goal. This is not stricly learning, it is memoising, so it is not an RL algorithm. However, you shouldnâ€™t discount it, because it is a very good and simple baseline. It can easily complete very sophisticated tasks if you give it enough time.\n",
    "\n",
    "Instead, let me introduce a slight variation to this algorithm called the Monte Carlo (MC) method. This lies at the heart of all modern RL algorithms so it is a great way to start.\n",
    "\n",
    "In short â€“ you can read more about this in Chapter 2 of the book â€“ MC methods attempt to randomly sample states and judge their value. Once you have sampled the states enough times then you can derive a strategy that follows the path of the next best value.\n",
    "\n",
    "Letâ€™s give it a try.\n",
    "\n",
    "### Generating trajectories\n",
    "Monte Carlo techniques operate by sampling the environment. In general, the idea is that if you can sample the environment enough times, you can begin to build a picture of the output, given any input. We can use this idea in RL. If we capture enough trajectories, where a trajectory is one full pass through an environment, then we can see which states are advantagous.\n",
    "\n",
    "To begin, I will create a class that is capable of generating trajectories. Here I pass in the environment, then in the run function I repeatedly step in the environment using a random action. I store each step in a list and return it to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloGeneration(object):\n",
    "  def __init__(self, env: object, max_steps: int = 1000, debug: bool = False):\n",
    "    self.env = env\n",
    "    self.max_steps = max_steps\n",
    "    self.debug = debug\n",
    "\n",
    "  def run(self) -> List:\n",
    "    buffer = []\n",
    "    n_steps = 0 # Keep track of the number of steps so I can bail out if it takes too long\n",
    "    state, _, _ = self.env.reset() # Reset environment back to start\n",
    "    terminal = False\n",
    "    while not terminal: # Run until terminal state\n",
    "      action = random.choice(self.env.action_space) # Random action. Try replacing this with Direction.EAST\n",
    "      next_state, reward, terminal = self.env.step(action) # Take action in environment\n",
    "      buffer.append((state, action, reward)) # Store the result\n",
    "      state = next_state # Ready for the next step\n",
    "      n_steps += 1\n",
    "      if n_steps >= self.max_steps:\n",
    "        if self.debug:\n",
    "          print(\"Terminated early due to large number of steps\")\n",
    "        terminal = True # Bail out if we've been working for too long\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_x___\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_x___\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "__x__\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "__x__\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "__x__\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_x___\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "__x__\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "_x___\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "_____\n",
      "x____\n",
      "_____\n",
      "_____\n",
      "____o\n",
      "\n",
      "Terminated early due to large number of steps\n",
      "['â®•', 'â¬†', 'â®•', 'â¬†', 'â¬‡', 'â¬…', 'â®•', 'â¬…', 'â¬…', 'â¬…']\n",
      "total reward: -10\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=True) # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env, max_steps=10, debug=True) # Instantiate the generation\n",
    "trajectory = generator.run() # Generate a trajectory\n",
    "print([t[1].value for t in trajectory]) # Print chosen actions\n",
    "print(f\"total reward: {sum([t[2] for t in trajectory])}\") # Print final reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Value\n",
    "Thereâ€™s an important quanity called the action value function. In summary, it is a measure of the value of taking a particular action, given all the experience. In other words, you can look at the previous trajectories, find out which of them lead to the highest values and look to use them again. See Chapter 2 in the book for more details.\n",
    "\n",
    "To generate an estimate of this value, generate a full trajectory, then look at how far away the agent is from the terminal states at all steps.\n",
    "\n",
    "So this means we need a class to generate a full trajectory, from start to termination. That code is below. First I create a new class that accepts the generator from before; Iâ€™ll use this later to generate the full trajectory.\n",
    "\n",
    "Then I create two fields to retain the experience observed by the agent. The first is recording the expected value at each state. This is the effectively the distance to the goal. The second is recording the number of times the agent has visited that state.\n",
    "\n",
    "Then I create a helper function to return a key for the dictionary (a.k.a. map) and an action value function to calculate the value of taking each action in each state. This is simply the average value over all visits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloExperiment(object):\n",
    "  def __init__(self, generator: MonteCarloGeneration):\n",
    "    self.generator = generator\n",
    "    self.values = defaultdict(float)\n",
    "    self.counts = defaultdict(float)\n",
    "\n",
    "  def _to_key(self, state, action):\n",
    "    return (state, action)\n",
    "  \n",
    "  def action_value(self, state, action) -> float:\n",
    "    key = self._to_key(state, action)\n",
    "    if self.counts[key] > 0:\n",
    "      return self.values[key] / self.counts[key]\n",
    "    else:\n",
    "      return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next I create a function to store this data after generating a full trajectory. There are several important parts of this function.\n",
    "\n",
    "The first is that Iâ€™m using reversed trajectories. I.e. Iâ€™m starting from the end and working backwards.\n",
    "\n",
    "The second is that Iâ€™m averaging the expected return over all visits. So this is reporting the value of an action, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloExperiment(MonteCarloExperiment):\n",
    "  def run_episode(self) -> None:\n",
    "    trajectory = self.generator.run() # Generate a trajectory\n",
    "    episode_reward = 0\n",
    "    for i, t in enumerate(reversed(trajectory)): # Starting from the terminal state\n",
    "      state, action, reward = t\n",
    "      key = self._to_key(state, action)\n",
    "      episode_reward += reward  # Add the reward to the buffer\n",
    "      self.values[key] += episode_reward # And add this to the value of this action\n",
    "      self.counts[key] += 1 # Increment counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Trajectory Generation\n",
    "Letâ€™s test this by setting some expectations. Weâ€™re reporting the value of taking an action on average. So on average, you would expect the value of taking the EAST action when next to the terminal state would be -1, because itâ€™s right there, itâ€™s a single step and therefore a single reward of -1 to get to the terminal state.\n",
    "\n",
    "However, other directions will not be -1, because the agent will continue to stumble around.\n",
    "\n",
    "So letâ€™s run a few episodes and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0:  [0.0, -1.0, 0.0, 0.0]\n",
      "Run 1:  [-171.0, -1.0, -173.0, 0.0]\n",
      "Run 2:  [-171.0, -1.0, -173.0, 0.0]\n",
      "Run 3:  [-171.0, -1.0, -173.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "env = SimpleGridWorld(debug=False) # Instantiate the environment - set the debug to true to see the actual movement of the agent.\n",
    "generator = MonteCarloGeneration(env=env, debug=True) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(4):\n",
    "  agent.run_episode()\n",
    "  print(f\"Run {i}: \", [agent.action_value(Point(3,0), d) for d in env.action_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see from above that yes, when choosing east from the point to the west of the terminal state the expected return is -1. But notice that the agent (probably) did not observe that result straight away, because it takes time to randomly select it. (Run it a few more times to see what happens, youâ€™ll see random changes)\n",
    "\n",
    "### Visualising the State Value Function\n",
    "The state value function is the average expected return for all actions. In general, you should see that the value increases the closer you get to the goal. But because of the random movement, especially far away from the goal, there will be a lot of noise.\n",
    "\n",
    "Below I create a helper function to plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-79.49 | -55.26 | -71.26 | -80.73 | -113.69 | \n",
      "-104.35 | -65.52 | -67.92 | -79.51 | -121.48 | \n",
      "-79.81 | -78.96 | -45.27 | -101.30 | -68.30 | \n",
      "-72.54 | -70.69 | -66.58 | -121.17 | -34.42 | \n",
      "-30.06 | -69.31 | -56.75 | -86.25 |    @   | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def state_value_2d(env, agent):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(env.height)):\n",
    "      for x in range(env.width):\n",
    "        if env.goal.x == x and env.goal.y == y:\n",
    "          res += \"   @  \"\n",
    "        else:\n",
    "          state_value = sum([agent.action_value(Point(x,y), d) for d in env.action_space]) / len(env.action_space)\n",
    "          res += f'{state_value:6.2f}'\n",
    "        res += \" | \"\n",
    "      res += \"\\n\"\n",
    "    return res\n",
    "\n",
    "print(state_value_2d(env, agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Optimal Policies\n",
    "A policy is a set of rules that an agent should follow. It is a strategy that works for that particular environment. You can now generate thousands of trajectories and track the expected value over time.\n",
    "\n",
    "With enough averaging, the expected values should present a clear picture of what the optimal policy is. See if you can see what it is?\n",
    "\n",
    "In the code below Iâ€™m instantiating all the previous code and then generating 1000 episodes. Then I print out the state value function for every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 999\n",
      "-109.82 | -107.14 | -100.47 | -93.93 | -89.37 | \n",
      "-104.69 | -102.09 | -95.65 | -90.31 | -85.09 | \n",
      "-101.46 | -97.07 | -90.80 | -78.55 | -70.43 | \n",
      "-95.90 | -90.49 | -82.77 | -62.72 | -46.04 | \n",
      "-94.54 | -84.92 | -75.32 | -47.73 |    @   | \n",
      "\n",
      "-109.82 | -107.14 | -100.47 | -93.93 | -89.37 | \n",
      "-104.69 | -102.09 | -95.65 | -90.31 | -85.09 | \n",
      "-101.46 | -97.07 | -90.80 | -78.55 | -70.43 | \n",
      "-95.90 | -90.49 | -82.77 | -62.72 | -46.04 | \n",
      "-94.54 | -84.92 | -75.32 | -47.73 |    @   | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = SimpleGridWorld() # Instantiate the environment\n",
    "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
    "agent = MonteCarloExperiment(generator=generator)\n",
    "for i in range(1000):\n",
    "  clear_output(wait=True)\n",
    "  agent.run_episode()\n",
    "  print(f\"Iteration: {i}\")\n",
    "  # print([agent.action_value(Point(0,4), d) for d in env.action_space]) # Uncomment this line to see the actual values for a particular state\n",
    "  print(state_value_2d(env, agent), flush=True)\n",
    "  time.sleep(0.05) # Uncomment this line if you want to see every episode\n",
    "\n",
    "print(state_value_2d(env, agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬‡ | â®• | â®• | â®• | â¬‡ | \n",
      "â®• | â®• | â¬‡ | â¬‡ | â¬‡ | \n",
      "â¬‡ | â¬‡ | â®• | â¬‡ | â¬‡ | \n",
      "â®• | â®• | â®• | â®• | â¬‡ | \n",
      "â®• | â®• | â®• | â®• | @ | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def argmax(a):\n",
    "    return max(range(len(a)), key=lambda x: a[x])\n",
    "\n",
    "def next_best_value_2d(env, agent):\n",
    "    res = \"\"\n",
    "    for y in reversed(range(env.height)):\n",
    "      for x in range(env.width):\n",
    "        if env.goal.x == x and env.goal.y == y:\n",
    "          res += \"@\"\n",
    "        else:\n",
    "          # Find the action that has the highest value\n",
    "          loc = argmax([agent.action_value(Point(x,y), d) for d in env.action_space])\n",
    "          res += f'{env.action_space[loc].value}'\n",
    "        res += \" | \"\n",
    "      res += \"\\n\"\n",
    "    return res\n",
    "\n",
    "print(next_best_value_2d(env, agent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
